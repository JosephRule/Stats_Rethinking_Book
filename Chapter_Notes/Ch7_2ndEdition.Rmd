---
title: "Ch7_2ndEdition"
output: html_document
---

```{r}
library(rethinking)
library(dagitty)
```

Causes of poor prediction:
(1) overfitting - learn too much from the data. Use regularization prior as a tool
(2) underfitting - learn too little from the data. Measure models' performance using information criteria

## 7.1 The Problem with Parameters

Confounding is another monster to avoid, but only is a problem if we're trying to interpret the model (less of a focus in this chapter). Here we mostly focus on prediction. 

Reminder that Rsqrd is proportion of variance explained (but is a bad measure). What we're discovering is that if you make the model more complicated you can get better model performance. This isn't always a good thing - we can overfit. 

### 7.1.1 More parameters alwyas improve fit
Learning to look out for overfitting. 

Some models' predictive ability do not necessarily improve w/ more predictors. The models we're talking about do though, so it's a problem for us.

Working with a brain size dataset
```{r}
sppnames <- c( "afarensis","africanus","habilis","boisei",
    "rudolfensis","ergaster","sapiens")
brainvolcc <- c( 438 , 452 , 612, 521, 752, 871, 1350 )
masskg <- c( 37.0 , 35.5 , 34.5 , 41.5 , 55.5 , 61.0 , 53.5 )
d <- data.frame( species=sppnames , brain=brainvolcc , mass=masskg )
```

Normalize variables and fit a model
```{r}
d$mass_std <- (d$mass - mean(d$mass))/sd(d$mass)
d$brain_std <- d$brain / max(d$brain)
```

```{r}
m7.1 <- map(
    alist(
        brain_std ~ dnorm( mu , exp(log_sigma) ),
        mu <- a + b*mass_std,
        a ~ dnorm( 0.5 , 1 ),
        b ~ dnorm( 0 , 10 ),
        log_sigma ~ dnorm( 0 , 1 )
    ), data=d )
```

Build a bunch more models with increasing polynomial values 
```{r}
m7.2 <- map(
    alist(
        brain_std ~ dnorm( mu , exp(log_sigma) ),
        mu <- a + b[1]*mass_std + b[2]*mass_std^2,
        a ~ dnorm( 0.5 , 1 ),
        b ~ dnorm( 0 , 10 ),
        log_sigma ~ dnorm( 0 , 1 )
    ), data=d , start=list(b=rep(0,2)) )
m7.3 <- quap(
    alist(
        brain_std ~ dnorm( mu , exp(log_sigma) ),
        mu <- a + b[1]*mass_std + b[2]*mass_std^2 +
                  b[3]*mass_std^3,
        a ~ dnorm( 0.5 , 1 ),
        b ~ dnorm( 0 , 10 ),
                log_sigma ~ dnorm( 0 , 1 )
    ), data=d , start=list(b=rep(0,3)) )
m7.4 <- quap(
    alist(
        brain_std ~ dnorm( mu , exp(log_sigma) ),
        mu <- a + b[1]*mass_std + b[2]*mass_std^2 +
                  b[3]*mass_std^3 + b[4]*mass_std^4,
        a ~ dnorm( 0.5 , 1 ),
        b ~ dnorm( 0 , 10 ),
        log_sigma ~ dnorm( 0 , 1 )
    ), data=d , start=list(b=rep(0,4)) )
m7.5 <- quap(
    alist(
        brain_std ~ dnorm( mu , exp(log_sigma) ),
        mu <- a + b[1]*mass_std + b[2]*mass_std^2 +
                  b[3]*mass_std^3 + b[4]*mass_std^4 +
                  b[5]*mass_std^5,
        a ~ dnorm( 0.5 , 1 ),
        b ~ dnorm( 0 , 10 ),
        log_sigma ~ dnorm( 0 , 1 )
    ), data=d , start=list(b=rep(0,5)) )
#had to give a hard sigma value below because the model exactly predicts all the points and sigma -> 0 when estimated (and kills things)
m7.6 <- quap(
    alist(
        brain_std ~ dnorm( mu , 0.001 ),
        mu <- a + b[1]*mass_std + b[2]*mass_std^2 +
                  b[3]*mass_std^3 + b[4]*mass_std^4 +
                  b[5]*mass_std^5 + b[6]*mass_std^6,
        a ~ dnorm( 0.5 , 1 ),
        b ~ dnorm( 0 , 10 )
    ), data=d , start=list(b=rep(0,6)) )
```


Steps to plot the posterior (review):
(1) Extract samples from the posterior
(2) Compute posterior predicitive distribution at several locations
(3) Summarize and plot
```{r}
post <- extract.samples(m7.1)
mass_seq <- seq( from=min(d$mass_std) , to=max(d$mass_std) , length.out=100 )
l <- link( m7.1 , data=list( mass_std=mass_seq ) )
mu <- apply( l , 2 , mean )
ci <- apply( l , 2 , PI )
plot( brain_std ~ mass_std , data=d )
lines( mass_seq , mu )
shade(ci, mass_seq)
```

He goes and plots all the increasing polynomial values. Plots become increasingly bad because there is no penalty for the model to swing wildly between predictions between points - it's being rewarded to do whatever it takes to fit the data. 

### 7.1.2 Too few parameters hurts too
Underfitting is the other beast
```{r}
m7.7 <- map(
  alist(
    brain_std ~ dnorm(mu, exp(log_sigma)),
    mu <- a,
    a ~ dnorm(0.5, 1),
    log_sigma ~ dnorm(0, 1)
  ), data=d
)
post <- extract.samples(m7.7)
mass_seq <- seq( from=min(d$mass_std) , to=max(d$mass_std) , length.out=100 )
l <- link( m7.7 , data=list( mass_std=mass_seq ) )
mu <- apply( l , 2 , mean )
ci <- apply( l , 2 , PI )
plot( brain_std ~ mass_std , data=d )
lines( mass_seq , mu )
shade(ci, mass_seq)
```
It just is taking in the mean value as the estimate. This makes sense, if you had no parameter and were asked to estimate the output variable, you'd do best by guessing the mean value. 

Another way to think about underfitting is that it's insensitive to sample. We could remove any one point from the sample and probably get the same regression line. Helpful to think of under/over fitting as being nonresponsive/responsive to the sample. 

Bias and variance tradeoff is a similar distinction. "bias" is related to underfitting and "variance" is related to overfitting. But terms mean a lot of things, so we talk about over/underfitting instead. 

## 7.2 Entrophy and Accuracy

We need to find a way to measure a model's performance. The steps ahead are:
(1) Information criteria - what is it?
(2) Deviance = approximation of relative distance from perfect accuracy
(3) We're really only interested in out-of-sample deviance 

### 7.2.1 Fiting the Weatherperson
We want to asses predictive power, but we need to think about what specifically goes into measuring this. What matters?
Two dimensions to think about:
(1) CBA - How much does it cost when we're wrong? How much do we win when we're right? 
(2) Accuracy in Context - We need a way to judge "accuracy" of the model that accounts for how mucha  model could possibly improve prediction. 

He goes into the weatherperson example.

1st weatherperson is confident in predicting rainy days, but gives 0.6 p of rain for days with sunlight
2nd weatherperson gives a 0% chance of rain for all days

Trying to judge their performance w/ the above criteria:
CBA -> need to make a judgement about if "hit rate" is valuable, or "unhappiness pts" (high cost of false negative)
Accuracy -> could we predict the exact sequence of days? (do we need to?)

Gets into the discussion about how the joint probability distribution is what we want because it shows up as the liklihood in our Bayes equation. 

### 7.2.2 Information and Uncertainty
In general asking "How much is our uncertainty reduced by learning an outcome?"
The measured decrease in uncertainty is the definition of information in this context.

We name some properties that we want from a measure of uncertainty:
(1) Continuous Measure - If not, any small change in any of the probabilities (like probability of rain) would result in massive change in uncertainty. 
(2) Measure should increase as number of possible events increase - We should be less certain when trying to predict sun, rain, snow than when only trying to estiamte sun or rain.
(3) The Measure should be additive - uncertainty over multiple events = sum(of uncertainties)

Information Entrophy to the resuce
H(p) = E - log(p) = -sum(p* log(p))
p is probability of events,
summed over the number of possible events

"The uncertainty contained in a probability distribution is the average log-probability of an event"
event = {weather type, bird species, DNA sequence}

Maximum Entrophy is interesting because you find the probability distributions that are most conservative w/ states of knowledge. "What is the least surprising distribution?" -> GLM 

### 7.2.3 From entrophy to accuracy
We use information to answer the question "how far is the model from a target?"

Divergence - The additional uncertainty induced by using probabilities from one distribution to describe another distribution (K-L divergence)

suppose true dist of events is (p1 = 0.3, p2 = 0.7) and modeled dist of events is (q1 = 0.25, q2 = 0.75)
"how much additional uncertainty have we introduced as a consequence of using q?)
```{r}
p = c(0.3, 0.7)
q = c(0.25, 0.75)

sum(p*log(p/q))
```

"The divergence is the average difference in log probability between the target (p) and model (q).
"This divergence is just the difference between two entrophies:  the entrophy of the target distribution p and the cross entrophy arising from using q to predict p"
Divergence = Cross Entrophy - Entrophy of Target
As q becomes more different from p, the divergence grows
When p = q, we know the probability of events & D = 0

There is an important note here that Divergence is dependent on direction. This is demonstrated by the travel to mars example. There is more uncertainty going Mars -> Earth because we haven't seen much water, so we're surprised to find it. However, going from Earth -> Mars, we wouldn't be terribly surprised to find either. 

If we use a distribution with high entrophy to approximate an unknown true distribution of events, we will reduce the distance to the truth (and the error)

A lot of build up to allow us to finally...
### 7.2.4 Estimating Divergence
Still building up to:
(1) How to measure distance from model -> target (K-L Divergence)
(2) How to estimate divergence (In real life)

He shows how to calcuate the log-probability score (is this the same as log-loss?)
There is mintuea with calculating this for a Bayesian model
For a Bayesian model, generate this score across the entire posterior (don't drop out parts of the posterior, use the whole thing)
*parameters & predictions have distributions
*find log average probability for observation i, where the average is taken over the posterior

Try out the rethiking function for the Log-Pointwise-Predictive-Density
```{r}
set.seed(1)
lppd(m7.1, n=1e4)
```

Each value returned is log-prob for each observation
Sum values to have total log-prob score for model&data

Larger values are better, they indicate larger average accuracy 
Deviance = -2*logprob -> smaller scores are better

Computers are sensitive to small numbers, so be careful when calculating and manipulating the deviance.



### Scoring the right data 

This score also improves w/ more complex models (booo). It's also not a measure on new data.
```{r}
set.seed(1)
sapply( list(m7.1,m7.2,m7.3,m7.4,m7.5,m7.6) , function(m) sum(lppd(m)) )
```

This introduces why we use training and test samples 

Below is code that's too much for my computer to run quickly
N <- 20
kseq <- 1:5
dev <- sapply( kseq , function(k) {
        print(k);
        r <- mcreplicate( 1e4 , sim_train_test( N=N, k=k ) , mc.cores=4 );
        c( mean(r[1,]) , mean(r[2,]) , sd(r[1,]) , sd(r[2,]) )
})

plot( 1:5 , dev[1,] , ylim=c( min(dev[1:2,])-5 , max(dev[1:2,])+10 ) ,
    xlim=c(1,5.1) , xlab="number of parameters" , ylab="deviance" ,
    pch=16 , col=rangi2 )
mtext( concat( "N = ",N ) )
points( (1:5)+0.1 , dev[2,] )
for ( i in kseq ) {
    pts_in <- dev[1,i] + c(-1,+1)*dev[3,i]
    pts_out <- dev[2,i] + c(-1,+1)*dev[4,i]
    lines( c(i,i) , pts_in , col=rangi2 )
    lines( c(i,i)+0.1 , pts_out )
}



Notes on figure 7.7
Both plots really shpw the deviance drop as you add the third variable
Deviance values between different samples is not comparable
Adding parameters improves deviance for in-sample, starts to hurt for out-sample

## 7.3 Regularization

We can add a skeptical prior to show the training rate (mentions using a beta prior)
He still seems spetical of a uniform prior though

Notes on Figure 7.9
Black = test deviance, always higher than train
Training dev gets worse w/ tight priors
  Skpetical prior protects from totally adapting sample
  However, it makes the train fit look worse (it is worse)
Test deviance gets better with tight priors
As prior gets more skeptical, harm done by overly complex model is reduced
As the sample goes to 100, the priors have much less effect

Later we'll do multilevel models, where we'll learn the strength of priors from the data. "Adaptive Regularization" - model judges how skeptical it should be. 

Ridge Regression - It's like a prior, but different. CI's are weird, but people don't interpret CI's correctly anyway.

## 7.4 Predicting Predictive Accuracy 

Two strategies: Cross-validation and information criteria 

Cross validation - 
Leave data out of the model fitting 
Leave one out cross validation (LOOCV) - costly, so we approximate it
Pareto-smoothed Importance Sampling Leave one out Cross validation (LOOIS)

Information critera is a theoretical estimate of relative out-of-sample K-L Divergence

AIC is an estimate of avg out-of-sample deviance, but we don't use it because it requires a lot of assumptions be true.

Dimensionality of posterior is natural measure of model's overfitting tendency

WAIC is the "Widely Applicable Information Criteria" 
*makes no assumption about the shape of the posterior
*approximation of out of sample deviance that converges to the LOOCV approximation in a large sample
*it can disagree on a small sample because it has a different target (K-L Divergence and not CV score)

LOOIS and WAIC are essentially the same in the context of ordinary linear regression. There is a lot more discussion in the chapter.

You can't compare deviance measures between models with different number of observations. Deviance is always higher for models trained with more points. I think this goes to the idea that there are just more opportunities for the model to be wrong. 
We also see that deviance is higher for models trained with flat priors. The graphs in figure 7.10 show that the deviance is a pretty good indicator for the number of parameters in the true model. deviance increases as more (unnecessary) parameters are used.

## 7.5 Using cross-validation and information criteria 

How do we compare model fits?
Cant just compare fit to sample (only favors more complex models)
Cant compare divergence (right measure, but still favors complexity)
Flat prior are bad assumptions - I think his point is either make them informed or uninformed
Yes - Use regularization priors 
Yes - Use WAIC and LOOIS

But we don't want to select from WAIC/LOOIS alone. We want to compare differences in accuracy between models - "how much better is this other model?"
Talking more about model comparison than model selection. IMO, model selection probably also has to do with the decision criteria

2 examples to talk about model comparison:
(1) Distinction beteen predictive & causal model puropse
(2) Reveal pointwise nature of cross_v and Inspection

### 7.5.1 Model mis-selection 
Cross validation does not solve any causal inference problems
Use m6.6, 6.7, 6.8 from previous chapter
```{r}
set.seed(11)
WAIC(m6.7)
```

1st value - out of sample deviance
remaining values are the components and standard errors 

Use compare to make life easy:
```{r}
set.seed(77)
compare(m6.6, m6.7, m6.8)
```

Note: LOOIS gives same results - use function LOO

WAIC - smaller value is better
pWAIC - penalty term of WAIC
dWAIC - change between WAIC and best WAIC
SE & dSE - compare to dWAIC to get interval

difference between model 7 (best) and model 8 is 40.9. Given dSE, the interval is (12.96, 67.04), so it's actually doing a lot better. 

All of this stuff tells us if we're predicting well. We know from the simulation how the data were generated, and it's not with this model. 

A variable can be causaly related to an outcome, but have little relative impact on it.

### 7.5.2 Something About Cebus
WAIC and LOOIS allow us to see which observations a model has trouble with. 

```{r}
data("Primates301")
d <- Primates301
```

```{r}
dag_7.1 <- dagitty( "dag {
    M -> L
    M -> B
    U -> M
    U -> L
    B -> L
}")

coordinates( dag_7.1 ) <- list( x=c(M=0, B=1, U=1, L=2) ,
                                  y=c(M=0, B=-1, U=1, L=0) )
plot(dag_7.1)
```
M = body mass
B = brain volume
L = longevity
U = unobserved variables 

```{r}
d$log_L <- scale( log(d$longevity) )
d$log_B <- scale( log(d$brain) )
d$log_M <- scale( log(d$body) )
d2 <- d[ complete.cases( d$log_L , d$log_M , d$log_B ) , ]
nrow(d2)
```

Fit different models here
```{r}
m7.8 <- map(
    alist(
        log_L ~ dnorm( mu , sigma ),
        mu <- a + bM*log_M + bB*log_B,
        a ~ dnorm(0,0.1),
        bM ~ dnorm(0,0.5),
        bB ~ dnorm(0,0.5),
        sigma ~ dexp(1)
) , data=d2 )
 m7.9 <-  map(
    alist(
        log_L ~ dnorm( mu , sigma ),
        mu <- a + bB*log_B,
        a ~ dnorm(0,0.1),
        bB ~ dnorm(0,0.5),
        sigma ~ dexp(1)
    ) , data=d2 )
m7.10 <- map(
    alist(
        log_L ~ dnorm( mu , sigma ),
        mu <- a + bM*log_M,
        a ~ dnorm(0,0.1),
        bM ~ dnorm(0,0.5),
        sigma ~ dexp(1)
    ) , data=d2 )
```

compare using waic
```{r}
set.seed(301)
compare(m7.8, m7.9, m7.10)
```

```{r}
plot(compare(m7.8, m7.9, m7.10))
```

```{r}
plot(coeftab(m7.8, m7.9, m7.10), pars=c("bM", "bB"))
```

The two results seem to give conlicting information. Turns out M & B are highly correlated, so posteriors of coefficients are uncertain.

Figure 7.12 is showing that different models are better at predicting longevity depending on the animal characteristics. However, this doesn't provide a causal explaination for why one model is better than another for different conditions. 

This is a long chapter.


## 7.7 Practice 
Note, some of these problems are labeled as Ch6 because this chapter of the book covers a lot of the 1st Edition Ch6 material.

Easy
6E1. State the three motivating criteria that define information entropy. Try to express each in your
own words.
*We want something that's continuous - we'd want to be able to figure out entrophy at any given point (though I'm not sure what the "points" are. What's a counter example here?)
*We want something that when there are more choices that the entrophy increases - looking to the weather example. The uncertainty in predicting rain vs sun should be less than predicting rain vs sun vs snow
*We want something that's additive - we want to be able to sum entrophy over many possible outcomes

6E2. Suppose a coin is weighted such that, when it is tossed and lands on a table, it comes up heads 70% of the time. What is the entropy of this coin?
```{r}
(0.7*log(0.7) + 0.3*log(0.3))*-1
```

6E3. Suppose a four-sided die is loaded such that, when tossed onto a table, it shows “1” 20%, “2” 25%, ”3” 25%, and ”4” 30% of the time. What is the entropy of this die?
```{r}
p <- c(0.2, 0.25, 0.25, 0.3)
-sum(p*log(p))
```

6E4. Suppose another four-sided die is loaded such that it never shows “4”. The other three sides show equally often. What is the entropy of this die?
```{r}
p <- c(1/3, 1/3, 1/3)
-sum(p*log(p))
```

Medium.

Medium 
6M1. Write down and compare the definitions of AIC, DIC, and WAIC. Which of these criteria is most general? Which assumptions are required to transform a more general criterion into a less general one?
AIC - -2*llpd + 2p (llpd = max liklihood function) - assume that priors are flat/overwhelmed by liklihood, posterior is multivariate Guassian, and sample size is much greater than number of parameters
DIC - assume that the posterior is multivariate Guassian
WAIC - most general case, 

6M2. Explain the difference between model selection and model averaging. What information is lost under model selection? What information is lost under model averaging?
Model selection is choosing one model that is performing/predicting/inferring the best. Model averaging means we're going to let a lot models vote on the outcome (there's a word for this I'm blanking on - "to bring many things together").
When choosing model selection we could miss out on models that pick up on edge cases in the data.
When choosing model averaging we're really sacrifincg any interpretability from the model.

6M3. When comparing models with an information criterion, why must all models be fit to exactly the same observations? What would happen to the information criterion values, if the models were fit to different numbers of observations? Perform some experiments, if you are not sure.
The calculation of deviance/entrophy/IC is completely dependent on the number of observations. As the number of observations increase, the entrophy also increases. However, comparing the entrophy of models trained from a different # of observations is comparing two completely different things. They don't scale the same, and you just shouldn't do it.

6M4. What happens to the effective number of parameters, as measured by DIC or WAIC, as a prior becomes more concentrated? Why? Perform some experiments, if you are not sure.
The WAIC measure is larger as a prior becomes less informed. 

6M5. Provide an informal explanation of why informative priors reduce overfitting.
A completely uninformed prior allows the model to learn completely from the data (leads to overfitting)

6M6. Provide an information explanation of why overly informative priors result in underfitting.
Overly-informed priors prevent the model from learning too much from the data (leads to an underfit model).

Hard.
```{r}
data("Howell1")
d <- Howell1
d$age <- (d$age - mean(d$age))/sd(d$age)
set.seed( 1000 )
i <- sample(1:nrow(d),size=nrow(d)/2)
d1 <- d[ i , ]
d2 <- d[ -i , ]
```

d1 is train, d2 is test

Fit multinomial models up to the 6th power

```{r}
m7.1H <- map(
    alist(
        height ~ dnorm( mu , sigma ),
        mu <- a + b[1]*age,
        a ~ dnorm( 40 , 20 ),
        b ~ dnorm( 0 , 10 ),
        sigma ~ dunif( 0 , 50 )
    ), data=d1)

m7.2H <- map(
    alist(
        height ~ dnorm( mu , sigma ),
        mu <- a + b[1]*age + b[2]*age^2,
        a ~ dnorm( 40 , 20 ),
        b ~ dnorm( 0 , 10 ),
        sigma ~ dunif( 0 , 50 )
    ), data=d1, start=list(b=rep(0,2))  )

m7.3H <- map(
    alist(
        height ~ dnorm( mu , sigma ),
        mu <- a + b[1]*age + b[2]*age^2 + b[3]*age^3,
        a ~ dnorm( 178 , 20 ),
        b ~ dnorm( 0 , 10 ),
        sigma ~ dunif( 0 , 50 )
    ), data=d1, start=list(b=rep(0,3))  )

m7.4H <- map(
    alist(
        height ~ dnorm( mu , sigma ),
        mu <- a + b[1]*age + b[2]*age^2 + b[3]*age^3 + b[4]*age^4,
        a ~ dnorm( 178 , 20 ),
        b ~ dnorm( 0 , 10 ),
        sigma ~ dunif( 0 , 50 )
    ), data=d1, , start=list(b=rep(0,4)) )

m7.5H <- map(
    alist(
        height ~ dnorm( mu , sigma ),
        mu <- a + b[1]*age + b[2]*age^2 + b[3]*age^3 + b[4]*age^4 + b[5]*age^5,
        a ~ dnorm( 178 , 20 ),
        b ~ dnorm( 0 , 10 ),
        sigma ~ dunif( 0 , 50 )
    ), data=d1, start=list(b=rep(0,5)) ) 

m7.6H <- map(
    alist(
        height ~ dnorm( mu , sigma ),
        mu <- a + b[1]*age + b[2]*age^2 + b[3]*age^3 + b[4]*age^4 + b[5]*age^5 + b[6]*age^6,
        a ~ dnorm( 178 , 20 ),
        b ~ dnorm( 0 , 10 ),
        sigma ~ dunif( 0 , 50 )
    ), data=d1, start=list(b=rep(0,6)) )
```


6H1. Compare the models above, using WAIC. Compare the model rankings, as well as the WAIC weights
```{r}
compare(m7.1H, m7.2H, m7.3H, m7.4H, m7.5H, m7.6H)
```
The most overfit model is fitting the best. The differences in the model deviances becomes very significant after the 2nd order

6H2. For each model, produce a plot with model averaged mean and 97% confidence interval of the mean, superimposed on the raw data. How do predicitons differ across models?

```{r}
mod_list <- c("m7.1H", "m7.2H", "m7.3H", "m7.4H", "m7.5H", "m7.6H")

for (m in mod_list) {
  temp_m <- get(m)
  post <- extract.samples(temp_m)
  age_seq <- seq( from=min(d$age) , to=max(d$age) , length.out=100 )
  l <- link( temp_m , data=list( age=age_seq ) )
  mu <- apply( l , 2 , mean )
  ci <- apply( l , 2 , PI )
  plot( height ~ age , data=d1, main = paste(m) )
  lines( age_seq , mu )
  shade(ci, age_seq)
}
```

The model starts really splining around and making overconfident fits at the high order. Between 2nd -> 3rd order is where things really go off the rails.
It's interesting though that the CI is much wider at the end for higher-order models than for the underfit model. Can this signal be generalized as a sign for overfitting? 

6H3. Now also plot the model averaged predictions, across all models. In what ways do the averaged predictions differ from the predictions of the model with the lowest WAIC? 

6H4. Compute the test-sample deviance for each model. This means calculating deviance, but using the data in d2 now. You can compute the log-liklihood of the height data with:
sum(dnorm(d2$height, mu, sigma, log=TRUE))
Where mu is the a vector of predicted means (based upon age values and MAP parameters) and sigma is the MAP standard deviation.

```{r}
test_deviance <- vector()
for (m in mod_list) {
  temp_m <- get(m)
  l <- link(temp_m, data=list(age=d2$age))
  mu <- apply( l , 2 , mean )
  s <- rep(temp_m@coef["sigma"], times = length(m))
  t_dev <- sum(dnorm(d2$height, mu, s, log=TRUE))
  test_deviance[m] <- t_dev 
  
}

-2*test_deviance
min(-2*test_deviance) - -2*test_deviance
```

I'm a bit lost what's going on here... So the estimate of deviance is great at the ends, gets a little messy for the in-between models. It doesn't idetify the same model as being optimal.
The WAIC calculations above found the best model to be that of the highest order.
This test deviance is finding that the best model is somewhere in between (4th order). I guess this is to show that the 5th and 6th order are even more overfit, and checking the deviance on the test data for the models is the way to check that. 


