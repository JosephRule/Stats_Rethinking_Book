---
title: "Ch7_2ndEdition"
output: html_document
---

```{r}
library(rethinking)
library(dagitty)
```

Causes of poor prediction:
(1) overfitting - learn too much from the data. Use regularization prior as a tool
(2) underfitting - learn too little from the data. Measure models' performance using information criteria

## 7.1 The Problem with Parameters

Confounding is another monster to avoid, but only is a problem if we're trying to interpret the model (less of a focus in this chapter). Here we mostly focus on prediction. 

Reminder that Rsqrd is proportion of variance explained (but is a bad measure). What we're discovering is that if you make the model more complicated you can get better model performance. This isn't always a good thing - we can overfit. 

### 7.1.1 More parameters alwyas improve fit
Learning to look out for overfitting. 

Some models' predictive ability do not necessarily improve w/ more predictors. The models we're talking about do though, so it's a problem for us.

Working with a brain size dataset
```{r}
sppnames <- c( "afarensis","africanus","habilis","boisei",
    "rudolfensis","ergaster","sapiens")
brainvolcc <- c( 438 , 452 , 612, 521, 752, 871, 1350 )
masskg <- c( 37.0 , 35.5 , 34.5 , 41.5 , 55.5 , 61.0 , 53.5 )
d <- data.frame( species=sppnames , brain=brainvolcc , mass=masskg )
```

Normalize variables and fit a model
```{r}
d$mass_std <- (d$mass - mean(d$mass))/sd(d$mass)
d$brain_std <- d$brain / max(d$brain)
```

```{r}
m7.1 <- map(
    alist(
        brain_std ~ dnorm( mu , exp(log_sigma) ),
        mu <- a + b*mass_std,
        a ~ dnorm( 0.5 , 1 ),
        b ~ dnorm( 0 , 10 ),
        log_sigma ~ dnorm( 0 , 1 )
    ), data=d )
```

Build a bunch more models with increasing polynomial values 
```{r}
m7.2 <- map(
    alist(
        brain_std ~ dnorm( mu , exp(log_sigma) ),
        mu <- a + b[1]*mass_std + b[2]*mass_std^2,
        a ~ dnorm( 0.5 , 1 ),
        b ~ dnorm( 0 , 10 ),
        log_sigma ~ dnorm( 0 , 1 )
    ), data=d , start=list(b=rep(0,2)) )
m7.3 <- quap(
    alist(
        brain_std ~ dnorm( mu , exp(log_sigma) ),
        mu <- a + b[1]*mass_std + b[2]*mass_std^2 +
                  b[3]*mass_std^3,
        a ~ dnorm( 0.5 , 1 ),
        b ~ dnorm( 0 , 10 ),
                log_sigma ~ dnorm( 0 , 1 )
    ), data=d , start=list(b=rep(0,3)) )
m7.4 <- quap(
    alist(
        brain_std ~ dnorm( mu , exp(log_sigma) ),
        mu <- a + b[1]*mass_std + b[2]*mass_std^2 +
                  b[3]*mass_std^3 + b[4]*mass_std^4,
        a ~ dnorm( 0.5 , 1 ),
        b ~ dnorm( 0 , 10 ),
        log_sigma ~ dnorm( 0 , 1 )
    ), data=d , start=list(b=rep(0,4)) )
m7.5 <- quap(
    alist(
        brain_std ~ dnorm( mu , exp(log_sigma) ),
        mu <- a + b[1]*mass_std + b[2]*mass_std^2 +
                  b[3]*mass_std^3 + b[4]*mass_std^4 +
                  b[5]*mass_std^5,
        a ~ dnorm( 0.5 , 1 ),
        b ~ dnorm( 0 , 10 ),
        log_sigma ~ dnorm( 0 , 1 )
    ), data=d , start=list(b=rep(0,5)) )
#had to give a hard sigma value below because the model exactly predicts all the points and sigma -> 0 when estimated (and kills things)
m7.6 <- quap(
    alist(
        brain_std ~ dnorm( mu , 0.001 ),
        mu <- a + b[1]*mass_std + b[2]*mass_std^2 +
                  b[3]*mass_std^3 + b[4]*mass_std^4 +
                  b[5]*mass_std^5 + b[6]*mass_std^6,
        a ~ dnorm( 0.5 , 1 ),
        b ~ dnorm( 0 , 10 )
    ), data=d , start=list(b=rep(0,6)) )
```


Steps to plot the posterior (review):
(1) Extract samples from the posterior
(2) Compute posterior predicitive distribution at several locations
(3) Summarize and plot
```{r}
post <- extract.samples(m7.1)
mass_seq <- seq( from=min(d$mass_std) , to=max(d$mass_std) , length.out=100 )
l <- link( m7.1 , data=list( mass_std=mass_seq ) )
mu <- apply( l , 2 , mean )
ci <- apply( l , 2 , PI )
plot( brain_std ~ mass_std , data=d )
lines( mass_seq , mu )
shade(ci, mass_seq)
```

He goes and plots all the increasing polynomial values. Plots become increasingly bad because there is no penalty for the model to swing wildly between predictions between points - it's being rewarded to do whatever it takes to fit the data. 

### 7.1.2 Too few parameters hurts too
Underfitting is the other beast
```{r}
m7.7 <- map(
  alist(
    brain_std ~ dnorm(mu, exp(log_sigma)),
    mu <- a,
    a ~ dnorm(0.5, 1),
    log_sigma ~ dnorm(0, 1)
  ), data=d
)
post <- extract.samples(m7.7)
mass_seq <- seq( from=min(d$mass_std) , to=max(d$mass_std) , length.out=100 )
l <- link( m7.7 , data=list( mass_std=mass_seq ) )
mu <- apply( l , 2 , mean )
ci <- apply( l , 2 , PI )
plot( brain_std ~ mass_std , data=d )
lines( mass_seq , mu )
shade(ci, mass_seq)
```
It just is taking in the mean value as the estimate. This makes sense, if you had no parameter and were asked to estimate the output variable, you'd do best by guessing the mean value. 

Another way to think about underfitting is that it's insensitive to sample. We could remove any one point from the sample and probably get the same regression line. Helpful to think of under/over fitting as being nonresponsive/responsive to the sample. 

Bias and variance tradeoff is a similar distinction. "bias" is related to underfitting and "variance" is related to overfitting. But terms mean a lot of things, so we talk about over/underfitting instead. 

## 7.2 Entrophy and Accuracy

We need to find a way to measure a model's performance. The steps ahead are:
(1) Information criteria - what is it?
(2) Deviance = approximation of relative distance from perfect accuracy
(3) We're really only interested in out-of-sample deviance 

### 7.2.1 Fiting the Weatherperson
We want to asses predictive power, but we need to think about what specifically goes into measuring this. What matters?
Two dimensions to think about:
(1) CBA - How much does it cost when we're wrong? How much do we win when we're right? 
(2) Accuracy in Context - We need a way to judge "accuracy" of the model that accounts for how mucha  model could possibly improve prediction. 

He goes into the weatherperson example.

1st weatherperson is confident in predicting rainy days, but gives 0.6 p of rain for days with sunlight
2nd weatherperson gives a 0% chance of rain for all days

Trying to judge their performance w/ the above criteria:
CBA -> need to make a judgement about if "hit rate" is valuable, or "unhappiness pts" (high cost of false negative)
Accuracy -> could we predict the exact sequence of days? (do we need to?)

Gets into the discussion about how the joint probability distribution is what we want because it shows up as the liklihood in our Bayes equation. 

### 7.2.2 Information and Uncertainty
In general asking "How much is our uncertainty reduced by learning an outcome?"
The measured decrease in uncertainty is the definition of information in this context.

We name some properties that we want from a measure of uncertainty:
(1) Continuous Measure - If not, any small change in any of the probabilities (like probability of rain) would result in massive change in uncertainty. 
(2) Measure should increase as number of possible events increase - We should be less certain when trying to predict sun, rain, snow than when only trying to estiamte sun or rain.
(3) The Measure should be additive - uncertainty over multiple events = sum(of uncertainties)

Information Entrophy to the resuce
H(p) = E - log(p) = -sum(p* log(p))
p is probability of events,
summed over the number of possible events

"The uncertainty contained in a probability distribution is the average log-probability of an event"
event = {weather type, bird species, DNA sequence}

Maximum Entrophy is interesting because you find the probability distributions that are most conservative w/ states of knowledge. "What is the least surprising distribution?" -> GLM 

### 7.2.3 From entrophy to accuracy
We use information to answer the question "how far is the model from a target?"

Divergence - The additional uncertainty induced by using probabilities from one distribution to describe another distribution (K-L divergence)

suppose true dist of events is (p1 = 0.3, p2 = 0.7) and modeled dist of events is (q1 = 0.25, q2 = 0.75)
"how much additional uncertainty have we introduced as a consequence of using q?)
```{r}
p = c(0.3, 0.7)
q = c(0.25, 0.75)

sum(p*log(p/q))
```

"The divergence is the average difference in log probability between the target (p) and model (q).
"This divergence is just the difference between two entrophies:  the entrophy of the target distribution p and the cross entrophy arising from using q to predict p"
Divergence = Cross Entrophy - Entrophy of Target
As q becomes more different from p, the divergence grows
When p = q, we know the probability of events & D = 0

There is an important note here that Divergence is dependent on direction. This is demonstrated by the travel to mars example. There is more uncertainty going Mars -> Earth because we haven't seen much water, so we're surprised to find it. However, going from Earth -> Mars, we wouldn't be terribly surprised to find either. 

If we use a distribution with high entrophy to approximate an unknown true distribution of events, we will reduce the distance to the truth (and the error)

A lot of build up to allow us to finally...
### 7.2.4 Estimating Divergence
Still building up to:
(1) How to measure distance from model -> target (K-L Divergence)
(2) How to estimate divergence (In real life)

He shows how to calcuate the log-probability score (is this the same as log-loss?)
There is mintuea with calculating this for a Bayesian model
For a Bayesian model, generate this score across the entire posterior (don't drop out parts of the posterior, use the whole thing)
*parameters & predictions have distributions
*find log average probability for observation i, where the average is taken over the posterior

Try out the rethiking function for the Log-Pointwise-Predictive-Density
```{r}
set.seed(1)
lppd(m7.1, n=1e4)
```

Each value returned is log-prob for each observation
Sum values to have total log-prob score for model&data

Larger values are better, they indicate larger average accuracy 
Deviance = -2*logprob -> smaller scores are better

Computers are sensitive to small numbers, so be careful when calculating and manipulating the deviance.



### Scoring the right data 

This score also improves w/ more complex models (booo). It's also not a measure on new data.
```{r}
set.seed(1)
sapply( list(m7.1,m7.2,m7.3,m7.4,m7.5,m7.6) , function(m) sum(lppd(m)) )
```

This introduces why we use training and test samples 

Below is code that's too much for my computer to run quickly
N <- 20
kseq <- 1:5
dev <- sapply( kseq , function(k) {
        print(k);
        r <- mcreplicate( 1e4 , sim_train_test( N=N, k=k ) , mc.cores=4 );
        c( mean(r[1,]) , mean(r[2,]) , sd(r[1,]) , sd(r[2,]) )
})

plot( 1:5 , dev[1,] , ylim=c( min(dev[1:2,])-5 , max(dev[1:2,])+10 ) ,
    xlim=c(1,5.1) , xlab="number of parameters" , ylab="deviance" ,
    pch=16 , col=rangi2 )
mtext( concat( "N = ",N ) )
points( (1:5)+0.1 , dev[2,] )
for ( i in kseq ) {
    pts_in <- dev[1,i] + c(-1,+1)*dev[3,i]
    pts_out <- dev[2,i] + c(-1,+1)*dev[4,i]
    lines( c(i,i) , pts_in , col=rangi2 )
    lines( c(i,i)+0.1 , pts_out )
}



Notes on figure 7.7
Both plots really shpw the deviance drop as you add the third variable
Deviance values between different samples is not comparable
Adding parameters improves deviance for in-sample, starts to hurt for out-sample

## 7.3 Regularization

We can add a skeptical prior to show the training rate (mentions using a beta prior)
He still seems spetical of a uniform prior though

Notes on Figure 7.9
Black = test deviance, always higher than train
Training dev gets worse w/ tight priors
  Skpetical prior protects from totally adapting sample
  However, it makes the train fit look worse (it is worse)
Test deviance gets better with tight priors
As prior gets more skeptical, harm done by overly complex model is reduced
As the sample goes to 100, the priors have much less effect

Later we'll do multilevel models, where we'll learn the strength of priors from the data. "Adaptive Regularization" - model judges how skeptical it should be. 

Ridge Regression - It's like a prior, but different. CI's are weird, but people don't interpret CI's correctly anyway.

## 7.4 Predicting Predictive Accuracy 

Two strategies: Cross-validation and information criteria 

Cross validation - 
Leave data out of the model fitting 
Leave one out cross validation (LOOCV) - costly, so we approximate it
Pareto-smoothed Importance Sampling Leave one out Cross validation (LOOIS)

Information critera is a theoretical estimate of relative out-of-sample K-L Divergence

AIC is an estimate of avg out-of-sample deviance, but we don't use it because it requires a lot of assumptions be true.

Dimensionality of posterior is natural measure of model's overfitting tendency

WAIC is the "Widely Applicable Information Criteria" 
*makes no assumption about the shape of the posterior
*approximation of out of sample deviance that converges to the LOOCV approximation in a large sample
*it can disagree on a small sample because it has a different target (K-L Divergence and not CV score)

LOOIS and WAIC are essentially the same in the context of ordinary linear regression. There is a lot more discussion in the chapter.

## 7.5 Using cross-validation and information criteria 

How do we compare model fits?
Cant just compare fit to sample (only favors more complex models)
Cant compare divergence (right measure, but still favors complexity)
Flat prior are bad assumptions - I think his point is either make them informed or uninformed
Yes - Use regularization priors 
Yes - Use WAIC and LOOIS

But we don't want to select from WAIC/LOOIS alone. We want to compare differences in accuracy between models - "how much better is this other model?"
Talking more about model comparison than model selection. IMO, model selection probably also has to do with the decision criteria

2 examples to talk about model comparison:
(1) Distinction beteen predictive & causal model puropse
(2) Reveal pointwise nature of cross_v and Inspection

### 7.5.1 Model mis-selection 
Cross validation does not solve any causal inference problems
Use m6.6, 6.7, 6.8 from previous chapter
```{r}
set.seed(11)
WAIC(m6.7)
```

1st value - out of sample deviance
remaining values are the components and standard errors 

Use compare to make life easy:
```{r}
set.seed(77)
compare(m6.6, m6.7, m6.8)
```

Note: LOOIS gives same results - use function LOO

WAIC - smaller value is better
pWAIC - penalty term of WAIC
dWAIC - change between WAIC and best WAIC
SE & dSE - compare to dWAIC to get interval

difference between model 7 (best) and model 8 is 40.9. Given dSE, the interval is (12.96, 67.04), so it's actually doing a lot better. 

All of this stuff tells us if we're predicting well. We know from the simulation how the data were generated, and it's not with this model. 

A variable can be causaly related to an outcome, but have little relative impact on it.

### 7.5.2 Something About Cebus
WAIC and LOOIS allow us to see which observations a model has trouble with. 

```{r}
data("Primates301")
d <- Primates301
```

```{r}
dag_7.1 <- dagitty( "dag {
    M -> L
    M -> B
    U -> M
    U -> L
    B -> L
}")

coordinates( dag_7.1 ) <- list( x=c(M=0, B=1, U=1, L=2) ,
                                  y=c(M=0, B=-1, U=1, L=0) )
plot(dag_7.1)
```
M = body mass
B = brain volume
L = longevity
U = unobserved variables 

```{r}
d$log_L <- scale( log(d$longevity) )
d$log_B <- scale( log(d$brain) )
d$log_M <- scale( log(d$body) )
d2 <- d[ complete.cases( d$log_L , d$log_M , d$log_B ) , ]
nrow(d2)
```

Fit different models here
```{r}
m7.8 <- map(
    alist(
        log_L ~ dnorm( mu , sigma ),
        mu <- a + bM*log_M + bB*log_B,
        a ~ dnorm(0,0.1),
        bM ~ dnorm(0,0.5),
        bB ~ dnorm(0,0.5),
        sigma ~ dexp(1)
) , data=d2 )
 m7.9 <-  map(
    alist(
        log_L ~ dnorm( mu , sigma ),
        mu <- a + bB*log_B,
        a ~ dnorm(0,0.1),
        bB ~ dnorm(0,0.5),
        sigma ~ dexp(1)
    ) , data=d2 )
m7.10 <- map(
    alist(
        log_L ~ dnorm( mu , sigma ),
        mu <- a + bM*log_M,
        a ~ dnorm(0,0.1),
        bM ~ dnorm(0,0.5),
        sigma ~ dexp(1)
    ) , data=d2 )
```

compare using waic
```{r}
set.seed(301)
compare(m7.8, m7.9, m7.10)
```

```{r}
plot(compare(m7.8, m7.9, m7.10))
```

```{r}
plot(coeftab(m7.8, m7.9, m7.10), pars=c("bM", "bB"))
```

The two results seem to give conlicting information. Turns out M & B are highly correlated, so posteriors of coefficients are uncertain.

Figure 7.12 is showing that different models are better at predicting longevity depending on the animal characteristics. However, this doesn't provide a causal explaination for why one model is better than another for different conditions. 

This is a long chapter.