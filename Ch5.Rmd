---
title: "Ch5"
output: html_document
---
```{r}
library(rethinking)
```


## Chapter 5

Discussion about multiple correlations.

Reasons to do multivariate regression:
*1. "control" for confounds.(mention of Simpson's Paradox)
*2. multiple causation - measure effects of multiple variables
*3. interactions - later topic

We'll be able to identify (1) spurious correlations and (2) masked correlations

### 5.1 Spurious association
Talking about if marriage rate and/or median marriage age cause divorce. Interesting that we may not be able to answer this question directly, but we do a tap dance & speak like we can.

*standardize the predictor is good practice

```{r}
data("WaffleDivorce")
d <- WaffleDivorce

#standardize the predictor
d$MedianAgeMarriage.s <- (d$MedianAgeMarriage-mean(d$MedianAgeMarriage)) / sd(d$MedianAgeMarriage)


#fit model
m5.1 <- map(
  alist(
    Divorce ~ dnorm(mu, sigma),
    mu <- a + bA * MedianAgeMarriage.s,
    a ~ dnorm(10,10),
    bA ~ dnorm(0,1),
    sigma ~ dunif(0,10)
  ), data = d)
precis(m5.1)
```

compute the percentile interval of the mean
```{r}
MAM.seq <- seq(from=-3, to=3.5, length.out=30)
mu <- link(m5.1, data = data.frame(MedianAgeMarriage.s=MAM.seq))
mu.PI <- apply(mu, 2, PI)

#plot
plot(Divorce ~ MedianAgeMarriage.s, data=d, col=rangi2)
abline(m5.1)
shade(mu.PI, MAM.seq)
```


```{r}
d$Marriage.s <- (d$Marriage - mean(d$Marriage))/sd(d$Marriage)

m5.2 <- map(
  alist(
    Divorce ~ dnorm(mu, sigma),
    mu <- a + bR * Marriage.s,
    a ~ dnorm(10,10),
    bR ~ dnorm(0,1),
    sigma ~ dunif(0,10)
  ), data = d
)
precis(m5.2)
```

```{r}
MAM.seq <- seq(from=-3, to=3.5, length.out=30)
mu <- link(m5.2, data = data.frame(Marriage.s=MAM.seq))
mu.PI <- apply(mu, 2, PI)

#plot
plot(Divorce ~ Marriage.s, data=d, col=rangi2)
abline(m5.2)
shade(mu.PI, MAM.seq)
```

But we're keeping the variables separate here...
What is the predictive value of a variable, once I already know all of the other predictor variables?

"rethinking" bit about the idea of controlling for a variable. This implies one variable has the causal effect (which may or may not be true)

#### 5.1.1 Multivariate notation

on matrix notation:
m = Xb
b is column vector of parameters
X is the design matrix
  *as many rows as data
  *as man columns as predictors + 1 (intercept)
m is the predicted means


#### 5.1.2 Fitting the model
```{r}
m5.3 <- map(
  alist(
    Divorce ~ dnorm(mu, sigma),
    mu <- a + bA * MedianAgeMarriage.s + bR * Marriage.s,
    a ~ dnorm(10,10),
    bA ~ dnorm(0,1),
    bR ~ dnorm(0,1),
    sigma ~ dunif(0,10)
  ), data = d
)
precis(m5.3)
```

```{r}
plot(precis(m5.3))
```

interpretation: "Once we know the median age at marriage for a State, there is little or no additional predictive power in also knowing the rate of marriage in that State."

#### 5.1.3 Plotting multivariate posteriors
*1 Predictor residual plots - look for fishy fits
*2 Conterfactual plots - show implied predictions for imaginary experiments in which the different predictor variables can be changed independently of one another.
*3 Posterior prediction plots - show model-based predictions against raw data, or otherwise display the error in prediction

##### 5.1.4 predictor residual plots
ave prediction error after predictors are included
when plotted against an outcome, we have a "bivariate regression that has already "controlled" for all of the other predictor variables"

Use the other predictor to model the residuals for other variable...
```{r}
#make the prediction model
m5.4 <- map(
  alist(
    Marriage.s ~ dnorm(mu, sigma),
    mu <- a + b*MedianAgeMarriage.s,
    a ~ dnorm(0,10),
    b ~ dnorm(0, 1),
    sigma ~ dunif(0, 10)
  ), data = d)

#compute residuals by subtracting observed marraige rates in each State from the predicted rate, based upon using age at marraige:

#comput expected value at MAP, for each State
mu <- coef(m5.4)['a'] + coef(m5.4)['b']*d$MedianAgeMarriage.s
#compute residual for each State
m.resid <- d$Marriage.s - mu
```

make hte plots
```{r}
plot(Marriage.s ~ MedianAgeMarriage.s, d, col=rangi2)
abline(m5.4)
#loop over states
for (i in 1:length(m.resid)) {
  x <- d$MedianAgeMarriage.s[i] #location of x segment
  y <- d$Marriage.s[i] #observed endpoint of line segment
  #draw line segment
  lines(c(x,x), c(mu[i], y), lwd = 0.5, col=col.alpha("black",0.7))
}
```

I need to review statistical anlaysis and remember what I'm looking for in these plots.


##### 5.1.3.2 Counterfactual plots
See how the predictions change as you change only one prediction at a time.

Show the impact of changes in Marriage.s on predictions (note: predictions and not on divorce rate itself)
```{r}
#prepare new counterfactual data
A.avg <- mean(d$MedianAgeMarriage.s)
R.seq <- seq(from =-3, to=3, length.out=30)
pred.data <- data.frame(
  Marriage.s=R.seq,
  MedianAgeMarriage.s=A.avg
)
#


#compute counterfactual mean divorce (mu)
mu <- link(m5.3, data=pred.data)
#"computes the value of each linear model at each sample for each case in the data using inverse link function"

mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

#simulate conterfactual divorce outcomes
R.sim <- sim(m5.3, data=pred.data, n=1e4)
```

Plot this
```{r}
R.PI <- apply(R.sim, 2, PI)

#display predictions, hiding raw data with type="n"
plot(Divorce ~ Marriage.s, data=d, type="n")
mtext("MedianAgeMarriage.s = 0")
lines(R.seq, mu.mean)
shade(mu.PI, R.seq)
shade(R.PI, R.seq)
```

Plot shows change in predicted mean across values of a single predictor, holding the other predictor constant at its mean value
I want to think of these just plots of the line, but I think the distinction has to do with holding the other variable constant at its mean value.

```{r}
output_var = "Divorce"
input_list = c("MedianAgeMarriage.s", "Marriage.s")
```

```{r}
#function to make counterfactual plots
#can't quite get this to work

counterfactual_plots <- function(data, output_var, input_list, lin_stan_model){
#loop through each input variable

  for (i in 1:length(input_list)){
    R.avg <- mean(select(data, input_list[i]))
    A.seq <- seq( from=-3, to=3.5, length.out=30) 

    temp.data <- data.frame( 
        R.avg=R.avg, 
        A.seq = A.seq 
    ) 

    mu <- link( lin_stan_model, data=temp.data) 
    mu.mean <- apply( mu, 2, mean)
    mu.PI <- apply( mu, 2, PI) 
    A.sim <- sim( lin_stan_model, data=temp.data, n=1e4) 
    A.PI <- apply( A.sim, 2, PI) 

    eqn = output_var + 
    plot( paste(output_var, "~ ", A.seq), data, type="n") 
    mtext( paste(input_list[1], "= 0")) 
    lines( A.seq, mu.mean) 
    shade( mu.PI, A.seq) 
    shade( A.PI, A.seq)
}
}
```

```{r}
counterfactual_plots(d, output_var, input_list, m5.3)
```


```{r}
R.avg <- mean( d$Marriage.s) 
A.seq <- seq( from=-3, to=3.5, length.out=30) 

pred.data2 <- data.frame( 
  Marriage.s=R.avg, 
  MedianAgeMarriage.s=A.seq 
) 

mu <- link( m5.3, data=pred.data2) 
mu.mean <- apply( mu, 2, mean) 
mu.PI <- apply( mu, 2, PI) 
A.sim <- sim( m5.3, data=pred.data2, n=1e4) 
A.PI <- apply( A.sim, 2, PI) 

plot( Divorce ~ MedianAgeMarriage.s, data=d, type="n") 
mtext( "Marriage.s = 0") 
lines( A.seq, mu.mean) 
shade( mu.PI, A.seq) 
shade( A.PI, A.seq)
```

Really need to think about the direction of causality. Plots may give unrealistic view of how changing a variable will affect the output variable.


 ##### 5.1.3.3 Posterior prediction plots
 Check the model fit against the observed data
 
 (1) Did the model fit correctly? 
 Does the model represent the data? Was the model supposed to represent the data?
 
 (2) How does the model fail?
 
 Start by simulating predictions, averaging over the posterior:
```{r}
#call link without specifiying new data
#so that it uses the original data
mu <- link(m5.3)

#summarize samples across cases
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

#simulate observations
#again, no new data, so uses original data
divorce.sim <- sim(m5.3, n=1e4)
divorce.PI <-  apply(divorce.sim, 2, PI)
```
 
 simpliest way to show model is to plot predictions against observed.
 
line shows perfect prediction and line segments for the confidence interval of each prediction
```{r}
plot( mu.mean ~ d$Divorce, col=rangi2, ylim=range(mu.PI), xlab="Observed divorce", ylab="Predicted divorce") 
abline( a=0, b=1, lty=2) 
for (i in 1:nrow(d)) 
  lines( rep(d$Divorce[i],2), c(mu.PI[1,i],mu.PI[2,i]), col=rangi2)
```
 
 So I'm under-predicting high divorce rate states, and over-predicting lower divorce rate states
 
```{r}
#compute residuals 
divorce.resid <- d$Divorce - mu.mean 
#get ordering by divorce rate 
o <- order(divorce.resid) 
#make the plot 
dotchart( divorce.resid[o], labels=d$Loc[o], xlim=c(-6,5), cex=0.6) 
abline( v=0, col=col.alpha("black",0.2)) 
for (i in 1:nrow(d)) { 
  j <- o[i] 
  #which State in order 
  lines( d$Divorce[j]-c(mu.PI[1,j],mu.PI[2,j]), rep(i,2))
  points(d$Divorce[j]-c(divorce.PI[1,j],divorce.PI[2,j]), rep(i,2), pch=3, cex=0.6, col="gray") 
  }
#Average prediction error (residuals) against number of Waffle House per capita, with superimposed regression of the two variables
```
 
 A neat aside about causal variable creating spurious relationships:
```{r}
N <- 100
x_real <- rnorm(N)
x_spur <- rnorm(N, x_real) #x_spur as Guassian with mean = x_real
y <- rnorm(N, x_real)
d <- data.frame(y, x_real, x_spur)
pairs(d)
```
 
```{r}
m <- lm(y ~ x_spur, data = d)
summary(m)
m <- lm(y ~ x_real, data = d)
summary(m)
m <- lm(y ~ x_real + x_spur, data = d)
summary(m)
```
 
 ### 5.2 Masked relationship
 Trying to measure the direct influences of multiple factors on an outcome, when none of those influences is apparant from bivariate relationships. (common for oppositely correlated predictors)
 
```{r}
data(milk)
d <- milk
str(milk)
dcc <- d[complete.cases(d),]
```

The question here is to what extent energy content of milk, measured here by kilocalories, is related to the percent of the brain mass that is neocortex. Neocortex is the gray, outer part of the brain that is particularly elaborated in mammals and especially primates. We’ll end up needing female body mass as well, to see the masking that hides the relationships among the variables.

Set up regression between kilocalories and neocortex percent
```{r}
m5.5 <- map(
  alist(
    kcal.per.g ~ dnorm(mu, sigma),
    mu <- a + b*neocortex.perc,
    a ~ dnorm(0, 100),
    b ~ dnorm(0, 1),
    sigma ~ dunif(0, 1)
  ), data = dcc)
precis(m5.5, digits = 3)
```
Very weak relationship (b is near zero and has a wide interval)


```{r}
np.seq <- 0:100 
pred.data <- data.frame( neocortex.perc=np.seq) 

mu <- link( m5.5, data=pred.data, n=1e4) 
mu.mean <- apply( mu, 2, mean) 
mu.PI <- apply( mu, 2, PI) 

plot( kcal.per.g ~ neocortex.perc, data=dcc, col=rangi2) 
lines( np.seq, mu.mean) 
lines( np.seq, mu.PI[1,], lty=2) 
lines( np.seq, mu.PI[2,], lty=2)
```
slopes really could be positive or negative, especially towrds the extremes

```{r}
dcc$log.mass <- log(dcc$mass)
```


also try log(mass)
```{r}
m5.6 <- map(
  alist(
    kcal.per.g ~ dnorm(mu, sigma),
    mu <- a + b*log.mass,
    a ~ dnorm(0, 100),
    b ~ dnorm(0, 1),
    sigma ~ dunif(0, 1)
  ), data = dcc)
precis(m5.6, digits = 3)
```

```{r}
np.seq <- 0:100 
pred.data <- data.frame( neocortex.perc=np.seq) 

mu <- link( m5.5, data=pred.data, n=1e4) 
mu.mean <- apply( mu, 2, mean) 
mu.PI <- apply( mu, 2, PI) 

plot( kcal.per.g ~ neocortex.perc, data=dcc, col=rangi2) 
lines( np.seq, mu.mean) 
lines( np.seq, mu.PI[1,], lty=2) 
lines( np.seq, mu.PI[2,], lty=2)
```

