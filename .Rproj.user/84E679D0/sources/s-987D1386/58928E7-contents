---
title: "Chapter 2"
output: html_document
---

vampire test example:
```{r}
# P(positive|V) = 95%
# the test detects vampirism 95% of the time
PrPV <- 0.95
# P(positive|mortal) = 1%
# the test inncorrectly diagnoses normal people with vamirism 1% of the time
PrPM <- 0.01
# P(V) = 0.1%
# Only 0.1% of the population is a vampire
PrV <- 0.001
#what is the probability that someone who tests positive is a vampire?
# P(V|positive) = P(P|V) * P(V) / P(P)

PrP <- PrPV*PrV + PrPM*(1-PrV) 
#this is the average probability of a positive test result

(PrVP <- PrPV*PrV/ PrP)
#8.7% chance that the suspect is actually a vampire
```

This is kind of confusing, so here's the logic that helps lead up to the equation:  
(1) In a population of 100,000 people, 100 of them are vampires.  
(2) Of the 100 who are vampires, 95 of them will test positive for vampirism.  
(3) Of the 99,900 mortals, 999 of them will test positive for vampirism.

What proportion of those who test positive for vampirism are vampires?
999 + 95 = 1094 test positive
95 / 1094 = 8.7% of those who test positive are varmpires.

### 3.1 Sampling from a grid-approximate posterior

Here we're generating the posterior using grid approximation:
```{r}
p_grid <- seq( from=0, to=1, length.out=1000)
prior <- rep( 1, 1000) 
likelihood <- dbinom( 6, size=9, prob=p_grid) 
posterior <- likelihood * prior 
posterior <- posterior/ sum(posterior)
```

```{r}
plot(posterior)
```


Now sample from this posterior distribution
```{r}
samples <- sample(p_grid, prob = posterior, size = 10000, replace = TRUE)
```
So, we're sampling from between 0 to 1, with the probability of each value being that of the posterior, so for this example the liklihood of sampling a 0 is near 0

```{r}
plot(samples)
#looking over the top of the posterior
```

```{r}
library(rethinking)
dens(samples)
```

What we're showing is that this is close to the ideal posterior computed using grid approximation....
(but we're sampling from that grid approximation posterior, so I don't really understand the point that's being made here)

### 3.2 Sampling to summarize
Once the model has produced a posterior, it's time to start interpreting...
This is where we start thinking about "how likely is that that the parameter is (above/below/between) x"

Questions about
(1) intervals of defined boundaries
(2) questions about intervals of defined probability mass
(3) questions about point estimates


#### 3.2.1 Intervals of defined boundaries
What is the probability that the proportion of water is less than 0.5?
```{r}
sum(posterior[p_grid < 0.5])
```

but we won't be able to rely on the grid approximation, so let's figure it out using the samples
```{r}
sum(samples < 0.5) / 10000
```

What about the probability that the proportion lies between 0.5 and 0.75?
```{r}
sum(samples > 0.5 & samples < 0.75) / 10000
```

##### Intervals of a defined mass

This is really talking about confidence (credibilty) intervals
```{r}
quantile(samples, 0.8)
#about 80% of mass is below 0.76
```

```{r}
quantile(samples, c(0.1, 0.9))
#80% of the mass falls between 0.45 and 0.81
# "percentile intervals"
```

Can be tricky because it doesn't always describe the shape of the distribution well.  
He gives this example where the posterior is skewed highly right (has a weird shape)
```{r}
p_grid <- seq( from=0, to=1, length.out=1000)
prior <- rep(1,1000) 
likelihood <- dbinom( 3, size=3, prob=p_grid) 
posterior <- likelihood * prior 
posterior <- posterior/ sum(posterior) 
samples <- sample( p_grid, size=1e4, replace=TRUE, prob=posterior)
```

```{r}
plot(posterior)
```

```{r}
PI(samples, prob = 0.5)
```

This doesn't really show that there's a higher concentration up near 1

Alternatively, use "Highest Posterior Density Interval" which answers this question more directly - where is the peak of the distribution?
```{r}
HPDI( samples, prob=0.5)
```
HDPI is more expensive and more sensitive to differences in simluated data

In the end, you're just trying to explain what the posterior describes, so if it's confusing, just plot it.

#### Point estimates

common point estimates are the 
maximum a posteriori (MAP)
```{r}
p_grid[which.max(posterior)]
chainmode(samples, adj=0.01)
```

```{r}
mean(samples)
median(samples)
```

If we wanted to get fancy, we could define a loss function and target the point estimate such that the loss function in minimized.

```{r}
#check the loss at 0.5
sum(posterior*abs(0.5 - p_grid))
```

apply this function over the entire distribution
```{r}
loss <- sapply( p_grid, function(d) sum( posterior*abs( d - p_grid)))
```

find at what value the loss is minimzed 
```{r}
p_grid[which.min(loss)]
```

### 3.3 Sampling to simulate prediction

four purposes for simulation:
(1) Model checking
fit model to real data, check "implied observations" and if fit worked correctly. I'm thinking this might include how sensitive is the model

(2) Software Validation  
Useful for model fitting software. Know what answer you should get and simulate results. Ken would do this for geotech things.

(3) Research Design  
Simulate observations to do a power analysis (soft fraud)  

(4) Forecasting  
simulate new predictions for new cases and future observations 

#### 3.3.1 Dummy data
simulate "world flipping" exercise using the binomial distiribution

on two tosses, what is prob 0, 1, 2 waters?
```{r}
dbinom(0:2, size = 2, prob=0.7)
```

randomly sample from the distribution:
```{r}
rbinom(1, size=2, prob=0.7)
```

```{r}
rbinom(10, size=2, prob=0.7)
```

Generate 100,000 dummy observations and plot the sample
```{r}
dummy_w <- rbinom(1e5, size=2, prob=0.7)
table(dummy_w)/1e5
```
very close to original probs

now do 9 tosses
```{r}
dummy_w <- rbinom( 1e5, size=9, prob=0.7)
simplehist( dummy_w, xlab="dummy water count")
```

I'm not sure what to make of this:
Rethinking: Sampling distributions. Many readers will already have seen simulated observations. Sampling distributions are the foundation of common non-Bayesian statistical traditions. In those approaches, inference about parameters is made through the sampling distribution. In this book, inference about parameters is never done directly through a sampling distribution. The posterior distribution is not sampled, but deduced logically. Then samples can be drawn from the posterior, as earlier in this chapter, to aid in inference. In neither case is “sampling” a physical act. In both cases, it’s just a mathematical device and produces only small world (Chapter 2) numbers.

I understand that bayesian inference is done by sampling the posterior, but I can't draw the distinction between that and "never (being) done directly through a sampling distribution"

#### 3.3.2 Model Checking

##### 3.3.2.1 Did the software work?
check whether the software worked by checking for correspondence between implied predictions and the data used to fit the model.  
There will be some disconnect betweeen data and simluation, especially when we get to multilevel models  

##### 3.3.2.2 Is the model adequate?
Where does the model fail? We can do this from creating random samples from the posterior distribution. 
Posterior predictive distribution - sampling distribution for each value of p in the posterior 

Take the globe flipping example.  
Use the posterior distribution (probability of water)  
Drawing lines at values of p (0.1, 0.2, 0.3, etc)
"Each of the ten parameter values implies a unique sampling distribution of predictions."  
Combining simulated observation distributions, each weighted by its posterior produces the posterior predictive distribution.  

Sampling in this way will give a distribution that accounts for more variability (and not just doubling down on what's near the max density)

So how do you actually do the calculations? To simulate predicted observations for a single value of p, say p = 0.6, you can use rbinom to generate random binomial samples:
```{r}
w <- rbinom(1e4, size=9, prob=0.6)
simplehist(w)
```

propogate the uncertainty into these predictions by replacing the value of 0.6 with samples from the posterior:  
```{r}
w <- rbinom(1e4, size=9, prob=samples)
simplehist(w)
```

I'm not incridebly clear on this point either, and I should spend more time with it- I think this is what we were doing in class.  

### 3.5 Practice

#### Easy
```{r}
p_grid <- seq( from=0, to=1, length.out=1000)
prior <- rep( 1, 1000) 
likelihood <- dbinom( 6, size=9, prob=p_grid)
posterior <- likelihood * prior 
posterior <- posterior/ sum(posterior)
set.seed(100) 
samples <- sample( p_grid, prob=posterior, size=1e4, replace=TRUE)
```

3E1. How much posterior probability lies below p = 0.2?  
```{r}
sum(posterior[p_grid < 0.2])
```

3E2. How much posterior probability lies above p = 0.8?  
```{r}
sum(posterior[p_grid > 0.8])
```

3E3. How much posterior probability lies between p = 0.2 and p = 0.8?
```{r}
sum(posterior[p_grid > 0.2 & p_grid < 0.8])
```

3E4. 20% of the posterior probability lies below which value of p? 
```{r}
quantile(samples, 0.2)
```

3E5. 20% of the posterior probability lies above which value of p?  
```{r}
quantile(samples, 0.8)
```

3E6. Which values of p contain the narrowest interval equal to 66% of the posterior probability?  
```{r}
HPDI(samples, prob=0.66)
```

3E7. Which values of p contain 66% of the posterior probability, assuming equal posterior probability both below and above the interval? 
```{r}
quantile(samples, c(0.165, 0.835))
```

#### Medium
3M1. Suppose the globe tossing data had turned out to be 8 water in 15 tosses. Construct the posterior distribution, using grid approximation. Use the same flat prior as before.  
```{r}
#setting up the grid
p_grid <- seq( from=0, to=1, length.out=1000)
#make a uniform prior
prior <- rep( 1, 1000) 
#assuming we saw 8 waters in 15 tosses
#what is the prob density assuming the probability = each value in the grid
#given that we saw 8 in 15, what is the prob density that the parameter is 0.01, 0.02, .. 0.5, ... etc
likelihood <- dbinom( 8, size=15, prob=p_grid)
#update the (uniform) prior with the liklihood 
posterior <- likelihood * prior 
#make the correction
posterior <- posterior/ sum(posterior)
plot(posterior)
```


3M2. Draw 10,000 samples from the grid approximation from above. Then use the samples to calculate the 90% HPDI for p.  
```{r}
set.seed(100) 
#sample from the grid, according to the relevant posterior probability
#it's more likely to pull from higher density areas
#sample 10,000 times
samples <- sample( p_grid, prob=posterior, size=1e4, replace=TRUE)
dens(samples)
HPDI(samples, prob=0.90)
```


3M3. Construct a posterior predictive check for this model and data. This means simulate the distribution of samples, averaging over the posterior uncertainty in p. What is the probability of observing 8 water in 15 tosses?
```{r}
w <- rbinom(1e4, size=15, prob=samples)
simplehist(w)
sum(w==8)/1e4
```

3M4. Using the posterior distribution constructed from the new (8/15) data, now calculate the probability of observing 6 water in 9 tosses.  
```{r}

```


3M5. Start over at 3M1, but now use a prior that is zero below p = 0.5 and a constant above p = 0.5. This corresponds to prior information that a majority of the Earth’s surface is water. Repeat each problem above and compare the inferences. What difference does the better prior make? If it helps, compare inferences (using both priors) to the true value p = 0.7.
```{r}
p_grid <- seq( from=0, to=1, length.out=1000)
prior2 <- c(rep(0, 500), rep(1,500))
likelihood2 <- dbinom( 8, size=15, prob=p_grid)
posterior2 <- likelihood * prior 
posterior2 <- posterior/ sum(posterior)
plot(posterior2)
set.seed(100) 
samples2 <- sample( p_grid, prob=posterior2, size=1e4, replace=TRUE)
dens(samples2)
sum(which(samples2==0.7))/1e4
```


#### Hard  
Introduction. The practice problems here all use the data below. These data indicate the gender (male=1, female=0) of officially reported first and second born children in 100 two-child families.  
```{r}
birth1 <- c(1,0,0,0,1,1,0,1,0,1,0,0,1,1,0,1,1,0,0,0,1,0,0,0,1,0,0,0,0,1,1,1,0,1,0,1,1,1,0,1,0,1,1,0,1,0,0,1,1,0,1,0,0,0,0,0,0,0,1,1,0,1,0,0,1,0,0,0,1,0,0,1,1,1,1,0,1,0,1,1,1,1,1,0,0,1,0,1,1,0,1,0,1,1,1,0,1,1,1,1) 
birth2 <- c(0,1,0,1,0,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,0,0,1,1,1,0,1,1,1,0,1,1,1,0,1,0,0,1,1,1,1,0,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,0,1,1,0,1,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0,1,0,0,1,1,0,0,0,1,1,1,0,0,0,0)
```

So for example, the first family in the data reported a boy (1) and then a girl (0). The second family reported a girl (0) and then a boy (1). The third family reported two girls. 
Use these vectors as data. So for example to compute the total number of boys born across all of these births, you could use:  

```{r}
sum(birth1) + sum(birth2)
```

3H1. Using grid approximation, compute the posterior distribution for the probability of a birth being a boy. Assume a uniform prior probability. Which parameter value maximizes the posterior probability?  
```{r}

```

3H2. Using the sample function, draw 10,000 random parameter values from the posterior distribution you calculated above. Use these samples to estimate the 50%, 89%, and 97% highest posterior density intervals. 
```{r}

```

3H3. Use rbinom to simulate 10,000 replicates of 200 births. You should end up with 10,000 numbers, each one a count of boys out of 200 births. Compare the distribution of predicted numbers of boys to the actual count in the data (111 boys out of 200 births). There are many good ways to visualize the simulations, but the dens command (part of the rethinking package) is probably the easiest way in this case. Does it look like the model fits the data well? That is, does the distribution of predictions include the actual observation as a central, likely outcome?  
```{r}

```

3H4. Now compare 10,000 counts of boys from 100 simulated first borns only to the number of boys in the first births, birth1. How does the model look in this light?  
```{r}

```

3H5. The model assumes that sex of first and second births are independent. To check this assumption, focus now on second births that followed female first borns. Compare 10,000 simulated counts of boys to only those second births that followed girls. To do this correctly, you need to count the number of first borns who were girls and simulate that many births, 10,000 times. Compare the counts of boys in your simulations to the actual observed count of boys following girls. How does the model look in this light? Any guesses what is going on in these data?
```{r}

```

